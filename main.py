import os, sys
from typing import Optional, Dict, Any, Tuple
import logging
from logging.handlers import RotatingFileHandler
import json
from fastapi import FastAPI, HTTPException, Request, Response
from contextlib import asynccontextmanager
from graphiti.graphiti_memory import GraphitiMemory
from graphiti.dependencies import get_mem
from graphiti.ontology import ENTITY_TYPES, EDGE_TYPES, EDGE_TYPE_MAP
from graphiti.context_builder import build_context_outline
from slowapi import Limiter, _rate_limit_exceeded_handler
from slowapi.util import get_remote_address
from slowapi.errors import RateLimitExceeded


# ---------- Logging: rotating file + console ----------
LOG_DIR = os.getenv("LOG_DIR", "./logs")
os.makedirs(LOG_DIR, exist_ok=True)
LOG_FILE = os.path.join(LOG_DIR, os.getenv("LOG_FILE", "middleware.log"))
LOG_LEVEL = os.getenv("LOG_LEVEL", "INFO").upper()
LOG_MAX_BYTES = int(os.getenv("LOG_MAX_BYTES", str(5 * 1024 * 1024)))   # 5 MB
LOG_BACKUP_COUNT = int(os.getenv("LOG_BACKUP_COUNT", "5"))

fmt = logging.Formatter("%(asctime)s %(levelname)s %(name)s - %(message)s")

_fh = RotatingFileHandler(LOG_FILE, maxBytes=LOG_MAX_BYTES, backupCount=LOG_BACKUP_COUNT, encoding="utf-8")
_fh.setFormatter(fmt)
_sh = logging.StreamHandler(sys.stdout)
_sh.setFormatter(fmt)

root = logging.getLogger()
root.setLevel(getattr(logging, LOG_LEVEL, logging.INFO))

# Replace handlers to avoid duplicates on reload
root.handlers = [_fh, _sh]

# Narrow app loggers (theyâ€™ll inherit handlers above)
logging.getLogger("shopware_ai.middleware").setLevel(root.level)
logging.getLogger("shopware_ai.gpt").setLevel(root.level)
logging.getLogger("shopware_ai.shopware").setLevel(root.level)

# ----------------------------------------
# FastAPI app with enhanced CORS, Helmet-like and Rate Limiting security
# ----------------------------------------
from handlers.gpt_handlers.gpt_agents.agent_coordinator import AgentCoordinator
from handlers.gpt_handlers.gpt_agents.search_agent import SearchAgent
from handlers.shopware_handlers.shopware_utils import ChatRequest, ChatResponse, SimpleHeaderInfo
from middleware_security.cors_config import setup_cors
from middleware_security.security import setup_security_headers

# Lifespan handler (startup/shutdown)
@asynccontextmanager
async def lifespan(app: FastAPI):
    mem = GraphitiMemory()
    await mem.initialize(build_indices=True)
    app.state.mem = mem
    try:
        yield
    finally:
        await app.state.mem.close()

app = FastAPI(
    title=os.getenv("API_TITLE", "WURM Shopware AI Agent Middleware"), 
    version=os.getenv("API_VERSION", "0.3.0"),
    docs_url="/docs" if os.getenv("ENVIRONMENT", "development") == "development" else None,
    redoc_url="/redoc" if os.getenv("ENVIRONMENT", "development") == "development" else None,
    lifespan=lifespan,
)

# Setup security middleware (order matters!)
setup_security_headers(app)
setup_cors(app)

# Include security test routes (only in development)
if os.getenv("ENVIRONMENT", "development") == "development":
    from middleware_security.test_routes import router as security_test_router
    app.include_router(security_test_router)
 
limiter = Limiter(key_func=get_remote_address)
app.state.limiter = limiter
app.add_exception_handler(RateLimitExceeded, _rate_limit_exceeded_handler)

async def _transcribe_audio_payload(payload: Dict[str, Any]) -> str:
    """
    Use transcripts generated by in-browser speech recognition. Or use OpenAI API here.
    """
    for key in ("transcript", "text", "recognizedText", "speechToText"):
        value = payload.get(key)
        if isinstance(value, str) and value.strip():
            return value.strip()
    raise HTTPException(
        status_code=400,
        detail="Voice payload missing browser transcription. Supply `transcript` or `text` field.",
    )


async def _normalize_customer_message(chat_request: ChatRequest) -> Tuple[str, bool]:
    """
    Return a cleaned natural-language message from the customer's payload.
    Supports optional voice payloads passed as JSON with base64 audio.
    """
    raw_message = chat_request.customerMessage or ""
    candidate = raw_message.strip()

    if candidate.startswith("{") and candidate.endswith("}"):
        try:
            parsed = json.loads(candidate)
        except json.JSONDecodeError:
            parsed = None
        if isinstance(parsed, dict):
            message_type = parsed.get("type", "").lower()
            if message_type in {"voice", "audio"} or any(k in parsed for k in ("audio_base64", "audio")):
                transcribed = await _transcribe_audio_payload(parsed)
                return transcribed, True
            if "text" in parsed:
                return str(parsed["text"]).strip(), False
    return candidate, False


async def _resolve_user_node(mem: GraphitiMemory, chat_request: ChatRequest) -> Optional[str]:
    """
    Attempt to locate an existing User node in the graph based on identifiers present in the request.
    Returns the user node UUID if found.
    """
    search_hints = [
        chat_request.customerNumber or chat_request.uuid,
        chat_request.contextToken,
        chat_request.salesChannelId,
        chat_request.languageId
    ] # add uuid or customerNumber if available

    for hint in filter(None, search_hints):
        nodes = await mem.search_nodes_rrf(hint, limit=5)
        for node in nodes:
            node_type = getattr(node, "type", "") or getattr(node, "kind", "")
            if str(node_type).lower() == "user":
                return node.uuid
    return None

# ------------------------------
# Routes
# ------------------------------
@app.get("/health")
async def health():
    logger = logging.getLogger("shopware_ai.middleware")
    logger.info("Health check")
    return {"status": "ok"}

# ------------------------------
# Test endpoints (dev-only)
# ------------------------------
from fastapi import Depends

#Initialize GraphitiMemory knowledge graph on startup
@app.on_event("startup")
async def startup_event():
    mem = app.state.mem
    if not mem or not mem.initialized:
        raise RuntimeError("GraphitiMemory not initialized on startup")
    logging.getLogger("shopware_ai.middleware").info("GraphitiMemory initialized and ready")

@app.post("/episodes/add")
async def add_episode_dev(payload: dict, mem: GraphitiMemory = Depends(get_mem)):
    """
    Dev helper: { "name": "foo", "text": "some content", "description": "desc" }
    """
    if "json" in payload:
        await mem.add_episode_json(
            name=payload.get("name", "dev-json"),
            payload=payload["json"],
            description=payload.get("description", "dev-json"),
            entity_types=ENTITY_TYPES,
            edge_types=EDGE_TYPES,
            edge_type_map=EDGE_TYPE_MAP,
        )
    else:
        await mem.add_episode_text(
            name=payload.get("name", "dev"),
            text=payload["text"],
            description=payload.get("description", "dev"),
            entity_types=ENTITY_TYPES, 
            edge_types=EDGE_TYPES, 
            edge_type_map=EDGE_TYPE_MAP,
        )
    return {"ok": True}

@app.get("/search")
async def search_dev(q: str, mem: GraphitiMemory = Depends(get_mem)):
    edges = await mem.search_edges(q, limit=12)
    nodes = await mem.search_nodes_rrf(q, limit=12)
    return {
        "edges": [getattr(e, "fact", None) for e in getattr(edges, "edges", [])],
        "nodes": [{"uuid": n.uuid, "name": getattr(n, "name", None)} for n in nodes],
    }

# ------------------------------
# Modify /chat to ingest & use context
# ------------------------------

agent_coordinator = AgentCoordinator()

@app.post("/chat", response_model=ChatResponse)
@limiter.limit("200/minute")
async def chat(
    request: Request,
    chat_request: ChatRequest,
    response: Response,
    mem: GraphitiMemory = Depends(get_mem),
):
    logging.getLogger("shopware_ai.middleware").info("REQUEST: %s", request)  # Remove in production

    # 0. The graph has multiple user nodes and products/actions linked to them. Find User's node in the graph (by uuid or customerNumber) and set it as a starting node for further search and episode adding.
    user_node_uuid = await _resolve_user_node(mem, chat_request)

    # 1. Format the input request. If voice, convert to text first. If text, validate, clean and use directly.
    normalized_message, was_voice = await _normalize_customer_message(chat_request)
    if normalized_message != chat_request.customerMessage:
        chat_request = chat_request.model_copy(update={"customerMessage": normalized_message})
    if not chat_request.customerMessage.strip():
        raise HTTPException(status_code=400, detail="Customer message is empty after normalization.")

    header_info = SimpleHeaderInfo(chat_request, user_node_uuid, was_voice)

    # 2. Use GraphitiMemory to ingest the user message as an episode (grows long-term memory)
    # episode_name = f"user:{user_node_uuid}" if user_node_uuid else f"user:{chat_request.languageId or 'default'}"
    # await mem.add_episode_text(
    #     name=episode_name,
    #     text=chat_request.customerMessage,
    #     description="user_message_voice" if was_voice else "user_message",
    #     entity_types=ENTITY_TYPES,
    #     edge_types=EDGE_TYPES,
    #     edge_type_map=EDGE_TYPE_MAP,
    # )
   
    #3. Use AgentCoordinator to analyze the message, determine intents, and plan actions (calls IntentAgent, RouterAgent, etc.)
    agent_coordinator.set_header_info(header_info)
    process_response = await agent_coordinator.process_chat_request(chat_request.customerMessage, mem)

    # 4. Choose the starting node and Build contextual outline from the graph (relevant products, cart items, user preferences, etc.)
    outline = await build_context_outline(
        mem,
        chat_request.customerMessage,
        limit=12,
        center_node_uuid=user_node_uuid,
    )

    return ChatResponse(
        ok=True,
        action="response",
        message=process_response["message"], #f"(demo) Context outline:\n{outline}",
        contextToken=chat_request.contextToken or "new-context-token",
        data={
            "note": "Replace this with GPT tool-use logic that calls Shopware APIs.",
            "userNodeUuid": user_node_uuid,
            "inputWasVoice": was_voice,
            "contextOutline": outline,
        },
    )
    

# ------------------------------
# Dev server (optional)
# ------------------------------
if __name__ == "__main__":
    import uvicorn
    uvicorn.run("main:app", host="0.0.0.0", port=int(os.getenv("PORT", 8000)), reload=True)
