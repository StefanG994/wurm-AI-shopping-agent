import os, sys
from datetime import datetime, timezone
from typing import Optional, Dict, Any, Tuple, List
import logging
from logging.handlers import RotatingFileHandler
import json
from fastapi import FastAPI, HTTPException, Request, Response
from contextlib import asynccontextmanager
from graphiti.graphiti_memory import GraphitiMemory
from graphiti.dependencies import get_mem
from graphiti.ontology import ENTITY_TYPES, EDGE_TYPES, EDGE_TYPE_MAP
from graphiti.context_builder import build_context_outline
from graphiti_core.nodes import EntityNode
from graphiti_core.edges import EntityEdge
from handlers.gpt_handlers.gpt_agents.intent_agent import IntentAgent
from slowapi import Limiter, _rate_limit_exceeded_handler
from slowapi.util import get_remote_address
from slowapi.errors import RateLimitExceeded


# ---------- Logging: rotating file + console ----------
LOG_DIR = os.getenv("LOG_DIR", "./logs")
os.makedirs(LOG_DIR, exist_ok=True)
LOG_FILE = os.path.join(LOG_DIR, os.getenv("LOG_FILE", "middleware.log"))
LOG_LEVEL = os.getenv("LOG_LEVEL", "INFO").upper()
LOG_MAX_BYTES = int(os.getenv("LOG_MAX_BYTES", str(5 * 1024 * 1024)))   # 5 MB
LOG_BACKUP_COUNT = int(os.getenv("LOG_BACKUP_COUNT", "5"))

fmt = logging.Formatter("%(asctime)s %(levelname)s %(name)s - %(message)s")

_fh = RotatingFileHandler(LOG_FILE, maxBytes=LOG_MAX_BYTES, backupCount=LOG_BACKUP_COUNT, encoding="utf-8")
_fh.setFormatter(fmt)
_sh = logging.StreamHandler(sys.stdout)
_sh.setFormatter(fmt)

root = logging.getLogger()
root.setLevel(getattr(logging, LOG_LEVEL, logging.INFO))

# Replace handlers to avoid duplicates on reload
root.handlers = [_fh, _sh]

# Narrow app loggers (theyâ€™ll inherit handlers above)
logging.getLogger("shopware_ai.middleware").setLevel(root.level)
logging.getLogger("shopware_ai.gpt").setLevel(root.level)
logging.getLogger("shopware_ai.shopware").setLevel(root.level)

# ----------------------------------------
# FastAPI app with enhanced CORS, Helmet-like and Rate Limiting security
# ----------------------------------------
from handlers.gpt_handlers.gpt_agents.search_agent import SearchAgent, ProductQuery
from handlers.shopware_handlers.shopware_utils import ChatRequest, ChatResponse, SimpleHeaderInfo
from middleware_security.cors_config import setup_cors
from middleware_security.security import setup_security_headers

# Lifespan handler (startup/shutdown)
@asynccontextmanager
async def lifespan(app: FastAPI):
    mem = GraphitiMemory()
    await mem.initialize(build_indices=True)
    app.state.mem = mem
    try:
        yield
    finally:
        await app.state.mem.close()

app = FastAPI(
    title=os.getenv("API_TITLE", "WURM Shopware AI Agent Middleware"), 
    version=os.getenv("API_VERSION", "0.3.0"),
    docs_url="/docs" if os.getenv("ENVIRONMENT", "development") == "development" else None,
    redoc_url="/redoc" if os.getenv("ENVIRONMENT", "development") == "development" else None,
    lifespan=lifespan,
)

# Setup security middleware (order matters!)
setup_security_headers(app)
setup_cors(app)

# Include security test routes (only in development)
if os.getenv("ENVIRONMENT", "development") == "development":
    from middleware_security.test_routes import router as security_test_router
    app.include_router(security_test_router)
 
limiter = Limiter(key_func=get_remote_address)
app.state.limiter = limiter
app.add_exception_handler(RateLimitExceeded, _rate_limit_exceeded_handler)

async def _transcribe_audio_payload(payload: Dict[str, Any]) -> str:
    """
    Use transcripts generated by in-browser speech recognition. Or use OpenAI API here.
    """
    for key in ("transcript", "text", "recognizedText", "speechToText"):
        value = payload.get(key)
        if isinstance(value, str) and value.strip():
            return value.strip()
    raise HTTPException(
        status_code=400,
        detail="Voice payload missing browser transcription. Supply `transcript` or `text` field.",
    )


async def _normalize_customer_message(chat_request: ChatRequest) -> Tuple[str, bool]:
    """
    Return a cleaned natural-language message from the customer's payload.
    Supports optional voice payloads passed as JSON with base64 audio.
    """
    raw_message = chat_request.customerMessage or ""
    candidate = raw_message.strip()

    if candidate.startswith("{") and candidate.endswith("}"):
        try:
            parsed = json.loads(candidate)
        except json.JSONDecodeError:
            parsed = None
        if isinstance(parsed, dict):
            message_type = parsed.get("type", "").lower()
            if message_type in {"voice", "audio"} or any(k in parsed for k in ("audio_base64", "audio")):
                transcribed = await _transcribe_audio_payload(parsed)
                return transcribed, True
            if "text" in parsed:
                return str(parsed["text"]).strip(), False
    return candidate, False


async def _resolve_user_node(mem: GraphitiMemory, chat_request: ChatRequest) -> Optional[str]:
    """
    Attempt to locate an existing User node in the graph based on identifiers present in the request.
    Returns the user node UUID if found.
    """
    search_hints = [
        chat_request.customerNumber or chat_request.uuid,
        chat_request.contextToken,
        chat_request.salesChannelId,
        chat_request.languageId
    ] # add uuid or customerNumber if available

    for hint in filter(None, search_hints):
        nodes = await mem.search_nodes_rrf(hint, limit=5)
        for node in nodes:
            node_type = getattr(node, "type", "") or getattr(node, "kind", "")
            if str(node_type).lower() == "user":
                return node.uuid
    return None


async def _persist_product_queries(
    mem: GraphitiMemory,
    *,
    group_id: str,
    product_queries: List[ProductQuery],
    user_node_uuid: Optional[str],
    intent_label: Optional[str],
) -> None:
    """Create ProductConcept, Brand, and ProductProperty nodes for the current request."""
    if not product_queries:
        return

    node_cache: Dict[str, EntityNode] = {}
    now = datetime.now(timezone.utc)
    graph = mem.client
    embedder = getattr(graph, "embedder", None) if graph else None

    async def _save_node(key: str, node: EntityNode) -> EntityNode:
        if embedder is not None:
            await node.generate_name_embedding(embedder)
        await mem.save_entity_node(node)
        node_cache[key] = node
        return node

    for query in product_queries:
        product_key = f"product::{group_id}::{query.label.lower()}"
        product_node = node_cache.get(product_key)
        if product_node is None:
            product_node = EntityNode(
                name=query.label,
                summary=intent_label or "Product requested by customer",
                group_id=group_id,
                labels=["ProductConcept"],
                attributes={"source": "customer_message"},
            )
            product_node = await _save_node(product_key, product_node)

    async def _save_edge(edge: EntityEdge) -> None:
        if embedder is not None:
            await edge.generate_embedding(embedder)
            await mem.save_entity_edge(edge)

        if user_node_uuid:
            await _save_edge(
                EntityEdge(
                    group_id=group_id,
                    source_node_uuid=user_node_uuid,
                    target_node_uuid=product_node.uuid,
                    created_at=now,
                    name="WANTS",
                    fact=f"User is interested in {query.label}",
                )
            )

        if query.brand:
            brand_key = f"brand::{query.brand.lower()}"
            brand_node = node_cache.get(brand_key)
            if brand_node is None:
                brand_node = EntityNode(
                    name=query.brand,
                    summary="Brand mentioned by customer",
                    group_id=group_id,
                    labels=["Brand"],
                    attributes={"source": "customer_message"},
                )
                brand_node = await _save_node(brand_key, brand_node)

            await _save_edge(
                EntityEdge(
                    group_id=group_id,
                    source_node_uuid=product_node.uuid,
                    target_node_uuid=brand_node.uuid,
                    created_at=now,
                    name="HAS_BRAND",
                    fact=f"{query.label} brand: {query.brand}",
                )
            )

        for prop in query.properties or []:
            prop_key = f"property::{prop.name.lower()}::{prop.value.lower()}"
            property_node = node_cache.get(prop_key)
            if property_node is None:
                property_node = EntityNode(
                    name=f"{prop.name}: {prop.value}",
                    summary="Product property requested by customer",
                    group_id=group_id,
                    labels=["ProductProperty"],
                    attributes={"property": prop.name, "value": prop.value},
                )
                property_node = await _save_node(prop_key, property_node)

            await _save_edge(
                EntityEdge(
                    group_id=group_id,
                    source_node_uuid=product_node.uuid,
                    target_node_uuid=property_node.uuid,
                    created_at=now,
                    name="HAS_PROPERTY",
                    fact=f"{query.label} requires {prop.name}={prop.value}",
                )
            )

    if intent_label:
        intent_key = f"intent::{group_id}::{intent_label.lower()}"
        intent_node = node_cache.get(intent_key)
        if intent_node is None:
            intent_node = EntityNode(
                name=intent_label,
                summary=intent_label,
                group_id=group_id,
                labels=["Intent"],
            )
            intent_node = await _save_node(intent_key, intent_node)

        if user_node_uuid:
            await _save_edge(
                EntityEdge(
                    group_id=group_id,
                    source_node_uuid=user_node_uuid,
                    target_node_uuid=intent_node.uuid,
                    created_at=now,
                    name="HAS_INTENT",
                    fact=intent_label,
                )
            )

# ------------------------------
# Routes
# ------------------------------
@app.get("/health")
async def health():
    logger = logging.getLogger("shopware_ai.middleware")
    logger.info("Health check")
    return {"status": "ok"}

# ------------------------------
# Test endpoints (dev-only)
# ------------------------------
from fastapi import Depends

#Initialize GraphitiMemory knowledge graph on startup
@app.on_event("startup")
async def startup_event():
    mem = app.state.mem
    if not mem or not mem.initialized:
        raise RuntimeError("GraphitiMemory not initialized on startup")
    logging.getLogger("shopware_ai.middleware").info("GraphitiMemory initialized and ready")

@app.post("/episodes/add")
async def add_episode_dev(payload: dict, mem: GraphitiMemory = Depends(get_mem)):
    """
    Dev helper: { "name": "foo", "text": "some content", "description": "desc" }
    """
    if "json" in payload:
        await mem.add_episode_json(
            name=payload.get("name", "dev-json"),
            payload=payload["json"],
            description=payload.get("description", "dev-json"),
            entity_types=ENTITY_TYPES,
            edge_types=EDGE_TYPES,
            edge_type_map=EDGE_TYPE_MAP,
        )
    else:
        await mem.add_episode_text(
            name=payload.get("name", "dev"),
            text=payload["text"],
            description=payload.get("description", "dev"),
            entity_types=ENTITY_TYPES, 
            edge_types=EDGE_TYPES, 
            edge_type_map=EDGE_TYPE_MAP,
        )
    return {"ok": True}

@app.get("/search")
async def search_dev(q: str, mem: GraphitiMemory = Depends(get_mem)):
    edges = await mem.search_edges(q, limit=12)
    nodes = await mem.search_nodes_rrf(q, limit=12)
    return {
        "edges": [getattr(e, "fact", None) for e in getattr(edges, "edges", [])],
        "nodes": [{"uuid": n.uuid, "name": getattr(n, "name", None)} for n in nodes],
    }

# ------------------------------
# Modify /chat to ingest & use context
# ------------------------------
from handlers.gpt_handler import _client

@app.post("/chat", response_model=ChatResponse)
@limiter.limit("200/minute")
async def chat(
    request: Request,
    chat_request: ChatRequest,
    response: Response,
    mem: GraphitiMemory = Depends(get_mem),
):
    logging.getLogger("shopware_ai.middleware").info("REQUEST: %s", request)  # Remove in production
    header_info = SimpleHeaderInfo(chat_request)

    # 0. The graph has multiple user nodes and products/actions linked to them. Find User's node in the graph (by uuid or customerNumber) and set it as a starting node for further search and episode adding.
    user_node_uuid = await _resolve_user_node(mem, chat_request)

    # 1. Format the input request. If voice, convert to text first. If text, validate, clean and use directly.
    normalized_message, was_voice = await _normalize_customer_message(chat_request)
    if normalized_message != chat_request.customerMessage:
        chat_request = chat_request.model_copy(update={"customerMessage": normalized_message})
    if not chat_request.customerMessage.strip():
        raise HTTPException(status_code=400, detail="Customer message is empty after normalization.")
    
    #2.0 Clasify general intent (PrimaryGoals)
    intent_agent = IntentAgent()
    primary_goals = await intent_agent.classify_general_intent(chat_request.customerMessage)
    logging.getLogger("shopware_ai.middleware").info("Primary Goals identified: %s", primary_goals.intent_list_order)
    logging.getLogger("shopware_ai.middleware").info("Message: %s", primary_goals.message)
    logging.getLogger("shopware_ai.middleware").info("Customer's goal: %s", primary_goals.goal)

    # # 2. Clarify user intent using Multi-Intent Classifier (from multi_intent.py)
    # intent_result = await intent_agent.classify_multi_intent(chat_request.customerMessage)
    # logging.getLogger("shopware_ai.middleware").info("All intentions in logical order: %s", intent_result.intent_list_order)
    # logging.getLogger("shopware_ai.middleware").info("Message: %s", intent_result.message)
    # logging.getLogger("shopware_ai.middleware").info("Customer's goal: %s", intent_result.goal)

    group_id = user_node_uuid or chat_request.uuid or (chat_request.languageId or "default")

    # 3. Use GraphitiMemory to ingest the user message as an episode (grows long-term memory)
    episode_name = f"user:{group_id}"
    await mem.add_episode_text(
        name=episode_name,
        text=f"user {user_node_uuid} goal: {primary_goals.goal}" if user_node_uuid else f"user:{chat_request.languageId or 'default'} goal: {primary_goals.goal}",
        description="user_message_voice" if was_voice else "user_message",
        entity_types=ENTITY_TYPES,
        edge_types=EDGE_TYPES,
        edge_type_map=EDGE_TYPE_MAP,
    )
    # # add json model dump as episode
    # await mem.add_episode_json(
    #     name=episode_name,
    #     payload=primary_goals.model_dump(),
    #     description="user_message_voice" if was_voice else "user_message",
    #     entity_types=ENTITY_TYPES,
    #     edge_types=EDGE_TYPES,
    #     edge_type_map=EDGE_TYPE_MAP,
    # )

    last_search_response: Any = None
    search_result_payload: Optional[Dict[str, Any]] = None
    pending_question: Optional[str] = None

    # Zvati funkciju koja ide po primary_goals.intent_list_order listi i poziva odgovarajuce agente
    for action in primary_goals.intent_list_order:
        logging.getLogger("shopware_ai.middleware").info("Processing action: %s", action)
        # Invoke the corresponding agent based on the action
        if action == "PRODUCTS":
            search_agent = SearchAgent()
            search_response = await search_agent.plan_search(
                chat_request.customerMessage,
                language_id=chat_request.languageId,
                header_info=header_info,
                parsed_intents=primary_goals.parsed_intents,
            )
            logging.getLogger("shopware_ai.middleware").info("Search Agent Response: %s", search_response)
            last_search_response = search_response
            if search_response.product_queries:
                await _persist_product_queries(
                    mem,
                    group_id=group_id,
                    product_queries=search_response.product_queries,
                    user_node_uuid=user_node_uuid,
                    intent_label=search_response.intent_summary or primary_goals.goal,
                )
            if search_response.shopware_response:
                search_result_payload = search_response.shopware_response
            if search_response.communication:
                pending_question = search_response.communication.message
            # Add json episode to memory
            await mem.add_episode_json(
                name=f"search_response_user:{group_id}",
                payload=search_response.model_dump(),
                description="search_agent_response",
                entity_types=ENTITY_TYPES,
                edge_types=EDGE_TYPES,
                edge_type_map=EDGE_TYPE_MAP,
            )


    # 4. Choose the starting node and Build contextual outline from the graph (relevant products, cart items, user preferences, etc.)
    outline = await build_context_outline(
        mem,
        chat_request.customerMessage,
        limit=12,
        center_node_uuid=user_node_uuid,
    )

    response_message = pending_question or f"(demo) Context outline:\n{outline}"

    data: Dict[str, Any] = {
        "note": "Replace this with GPT tool-use logic that calls Shopware APIs.",
        "userNodeUuid": user_node_uuid,
        "inputWasVoice": was_voice,
        "intent": primary_goals.model_dump(),
    }
    if search_result_payload is not None:
        data["searchResult"] = search_result_payload
    if last_search_response is not None:
        data["searchAction"] = last_search_response.action
        data["searchPayload"] = last_search_response.payload
    if pending_question:
        data["pendingQuestion"] = pending_question

    return ChatResponse(
        ok=True,
        action="response",
        message=response_message,
        contextToken=chat_request.contextToken or "new-context-token",
        data=data,
    )
    

# ------------------------------
# Dev server (optional)
# ------------------------------
if __name__ == "__main__":
    import uvicorn
    uvicorn.run("main:app", host="0.0.0.0", port=int(os.getenv("PORT", 8000)), reload=True)
