import os, sys
from typing import Optional, Dict, Any, Tuple
import logging
from logging.handlers import RotatingFileHandler
import json
from fastapi import FastAPI, HTTPException, Request, Response
from contextlib import asynccontextmanager
from graphiti.graphiti_memory import GraphitiMemory
from graphiti.dependencies import get_mem
from graphiti.ontology import ENTITY_TYPES, EDGE_TYPES, EDGE_TYPE_MAP
from graphiti.context_builder import build_context_outline
from handlers.gpt_handlers.gpt_agents.intent_agent import IntentAgent
from slowapi import Limiter, _rate_limit_exceeded_handler
from slowapi.util import get_remote_address
from slowapi.errors import RateLimitExceeded

from handlers.shopware_handlers.shopware_product_client import ProductClient  


# ---------- Logging: rotating file + console ----------
LOG_DIR = os.getenv("LOG_DIR", "./logs")
os.makedirs(LOG_DIR, exist_ok=True)
LOG_FILE = os.path.join(LOG_DIR, os.getenv("LOG_FILE", "middleware.log"))
LOG_LEVEL = os.getenv("LOG_LEVEL", "INFO").upper()
LOG_MAX_BYTES = int(os.getenv("LOG_MAX_BYTES", str(5 * 1024 * 1024)))   # 5 MB
LOG_BACKUP_COUNT = int(os.getenv("LOG_BACKUP_COUNT", "5"))

fmt = logging.Formatter("%(asctime)s %(levelname)s %(name)s - %(message)s")

_fh = RotatingFileHandler(LOG_FILE, maxBytes=LOG_MAX_BYTES, backupCount=LOG_BACKUP_COUNT, encoding="utf-8")
_fh.setFormatter(fmt)
_sh = logging.StreamHandler(sys.stdout)
_sh.setFormatter(fmt)

root = logging.getLogger()
root.setLevel(getattr(logging, LOG_LEVEL, logging.INFO))

# Replace handlers to avoid duplicates on reload
root.handlers = [_fh, _sh]

# Narrow app loggers (theyâ€™ll inherit handlers above)
logging.getLogger("shopware_ai.middleware").setLevel(root.level)
logging.getLogger("shopware_ai.gpt").setLevel(root.level)
logging.getLogger("shopware_ai.shopware").setLevel(root.level)

# ----------------------------------------
# FastAPI app with enhanced CORS, Helmet-like and Rate Limiting security
# ----------------------------------------
from handlers.gpt_handlers.gpt_agents.search_agent import SearchAgent
from handlers.shopware_handlers.shopware_utils import ChatRequest, ChatResponse, SimpleHeaderInfo
from middleware_security.cors_config import setup_cors
from middleware_security.security import setup_security_headers

# Lifespan handler (startup/shutdown)
@asynccontextmanager
async def lifespan(app: FastAPI):
    mem = GraphitiMemory()
    await mem.initialize(build_indices=True)
    app.state.mem = mem
    try:
        yield
    finally:
        await app.state.mem.close()

app = FastAPI(
    title=os.getenv("API_TITLE", "WURM Shopware AI Agent Middleware"), 
    version=os.getenv("API_VERSION", "0.3.0"),
    docs_url="/docs" if os.getenv("ENVIRONMENT", "development") == "development" else None,
    redoc_url="/redoc" if os.getenv("ENVIRONMENT", "development") == "development" else None,
    lifespan=lifespan,
)

# Setup security middleware (order matters!)
setup_security_headers(app)
setup_cors(app)

# Include security test routes (only in development)
if os.getenv("ENVIRONMENT", "development") == "development":
    from middleware_security.test_routes import router as security_test_router
    app.include_router(security_test_router)
 
limiter = Limiter(key_func=get_remote_address)
app.state.limiter = limiter
app.add_exception_handler(RateLimitExceeded, _rate_limit_exceeded_handler)

async def _transcribe_audio_payload(payload: Dict[str, Any]) -> str:
    """
    Use transcripts generated by in-browser speech recognition. Or use OpenAI API here.
    """
    for key in ("transcript", "text", "recognizedText", "speechToText"):
        value = payload.get(key)
        if isinstance(value, str) and value.strip():
            return value.strip()
    raise HTTPException(
        status_code=400,
        detail="Voice payload missing browser transcription. Supply `transcript` or `text` field.",
    )


async def _normalize_customer_message(chat_request: ChatRequest) -> Tuple[str, bool]:
    """
    Return a cleaned natural-language message from the customer's payload.
    Supports optional voice payloads passed as JSON with base64 audio.
    """
    raw_message = chat_request.customerMessage or ""
    candidate = raw_message.strip()

    if candidate.startswith("{") and candidate.endswith("}"):
        try:
            parsed = json.loads(candidate)
        except json.JSONDecodeError:
            parsed = None
        if isinstance(parsed, dict):
            message_type = parsed.get("type", "").lower()
            if message_type in {"voice", "audio"} or any(k in parsed for k in ("audio_base64", "audio")):
                transcribed = await _transcribe_audio_payload(parsed)
                return transcribed, True
            if "text" in parsed:
                return str(parsed["text"]).strip(), False
    return candidate, False


async def _resolve_user_node(mem: GraphitiMemory, chat_request: ChatRequest) -> Optional[str]:
    """
    Attempt to locate an existing User node in the graph based on identifiers present in the request.
    Returns the user node UUID if found.
    """
    search_hints = [
        chat_request.customerNumber or chat_request.uuid,
        chat_request.contextToken,
        chat_request.salesChannelId,
        chat_request.languageId
    ] # add uuid or customerNumber if available

    for hint in filter(None, search_hints):
        nodes = await mem.search_nodes_rrf(hint, limit=5)
        for node in nodes:
            node_type = getattr(node, "type", "") or getattr(node, "kind", "")
            if str(node_type).lower() == "user":
                return node.uuid
    return None

# ------------------------------
# Routes
# ------------------------------
@app.get("/health")
async def health():
    logger = logging.getLogger("shopware_ai.middleware")
    logger.info("Health check")
    return {"status": "ok"}

# ------------------------------
# Test endpoints (dev-only)
# ------------------------------
from fastapi import Depends

#Initialize GraphitiMemory knowledge graph on startup
@app.on_event("startup")
async def startup_event():
    mem = app.state.mem
    if not mem or not mem.initialized:
        raise RuntimeError("GraphitiMemory not initialized on startup")
    logging.getLogger("shopware_ai.middleware").info("GraphitiMemory initialized and ready")

@app.post("/episodes/add")
async def add_episode_dev(payload: dict, mem: GraphitiMemory = Depends(get_mem)):
    """
    Dev helper: { "name": "foo", "text": "some content", "description": "desc" }
    """
    if "json" in payload:
        await mem.add_episode_json(
            name=payload.get("name", "dev-json"),
            payload=payload["json"],
            description=payload.get("description", "dev-json"),
            entity_types=ENTITY_TYPES,
            edge_types=EDGE_TYPES,
            edge_type_map=EDGE_TYPE_MAP,
        )
    else:
        await mem.add_episode_text(
            name=payload.get("name", "dev"),
            text=payload["text"],
            description=payload.get("description", "dev"),
            entity_types=ENTITY_TYPES, 
            edge_types=EDGE_TYPES, 
            edge_type_map=EDGE_TYPE_MAP,
        )
    return {"ok": True}

@app.get("/search")
async def search_dev(q: str, mem: GraphitiMemory = Depends(get_mem)):
    edges = await mem.search_edges(q, limit=12)
    nodes = await mem.search_nodes_rrf(q, limit=12)
    return {
        "edges": [getattr(e, "fact", None) for e in getattr(edges, "edges", [])],
        "nodes": [{"uuid": n.uuid, "name": getattr(n, "name", None)} for n in nodes],
    }

# ------------------------------
# Modify /chat to ingest & use context
# ------------------------------
from handlers.gpt_handler import _client

@app.post("/chat", response_model=ChatResponse)
@limiter.limit("200/minute")
async def chat(
    request: Request,
    chat_request: ChatRequest,
    response: Response,
    mem: GraphitiMemory = Depends(get_mem),
):
    logging.getLogger("shopware_ai.middleware").info("REQUEST: %s", request)  # Remove in production
    header_info = SimpleHeaderInfo(chat_request)

    # 0. The graph has multiple user nodes and products/actions linked to them. Find User's node in the graph (by uuid or customerNumber) and set it as a starting node for further search and episode adding.
    user_node_uuid = await _resolve_user_node(mem, chat_request)

    # 1. Format the input request. If voice, convert to text first. If text, validate, clean and use directly.
    normalized_message, was_voice = await _normalize_customer_message(chat_request)
    if normalized_message != chat_request.customerMessage:
        chat_request = chat_request.model_copy(update={"customerMessage": normalized_message})
    if not chat_request.customerMessage.strip():
        raise HTTPException(status_code=400, detail="Customer message is empty after normalization.")
    
    #2.0 Clasify general intent (PrimaryGoals)
    intent_agent = IntentAgent()
    primary_goals = await intent_agent.classify_general_intent(chat_request.customerMessage)
    logging.getLogger("shopware_ai.middleware").info("Primary Goals identified: %s", primary_goals.intent_list_order)
    logging.getLogger("shopware_ai.middleware").info("Message: %s", primary_goals.message)
    logging.getLogger("shopware_ai.middleware").info("Customer's goal: %s", primary_goals.goal)

    # # 2. Clarify user intent using Multi-Intent Classifier (from multi_intent.py)
    # intent_result = await intent_agent.classify_multi_intent(chat_request.customerMessage)
    # logging.getLogger("shopware_ai.middleware").info("All intentions in logical order: %s", intent_result.intent_list_order)
    # logging.getLogger("shopware_ai.middleware").info("Message: %s", intent_result.message)
    # logging.getLogger("shopware_ai.middleware").info("Customer's goal: %s", intent_result.goal)

    # 3. Use GraphitiMemory to ingest the user message as an episode (grows long-term memory)
    episode_name = f"user:{user_node_uuid}" if user_node_uuid else f"user:{chat_request.languageId or 'default'}"
    await mem.add_episode_text(
        name=episode_name,
        text=f"user {user_node_uuid} goal: {primary_goals.goal}" if user_node_uuid else f"user:{chat_request.languageId or 'default'} goal: {primary_goals.goal}",
        description="user_message_voice" if was_voice else "user_message",
        entity_types=ENTITY_TYPES,
        edge_types=EDGE_TYPES,
        edge_type_map=EDGE_TYPE_MAP,
    )
    # add json model dump as episode
    await mem.add_episode_json(
        name=episode_name,
        payload=primary_goals.model_dump(),
        description="user_message_voice" if was_voice else "user_message",
        entity_types=ENTITY_TYPES,
        edge_types=EDGE_TYPES,
        edge_type_map=EDGE_TYPE_MAP,
    )

    # Zvati funkciju koja ide po primary_goals.intent_list_order listi i poziva odgovarajuce agente
    for action in primary_goals.intent_list_order:
        logging.getLogger("shopware_ai.middleware").info("Processing action: %s", action)
        # Invoke the corresponding agent based on the action
        if action == "PRODUCTS":
            search_agent = SearchAgent()
            search_response = await search_agent.plan_search(
                chat_request.customerMessage,
                language_id=chat_request.languageId,
            )
            logging.getLogger("shopware_ai.middleware").info("Search Agent Response: %s", search_response)
            # Add json episode to memory
            await mem.add_episode_json(
                name=f"search_response_user:{user_node_uuid}" if user_node_uuid else f"search_response_user:{chat_request.languageId or 'default'}",
                payload=search_response.model_dump(),
                description="search_agent_response",
                entity_types=ENTITY_TYPES,
                edge_types=EDGE_TYPES,
                edge_type_map=EDGE_TYPE_MAP,
            )


    # 4. Choose the starting node and Build contextual outline from the graph (relevant products, cart items, user preferences, etc.)
    outline = await build_context_outline(
        mem,
        chat_request.customerMessage,
        limit=12,
        center_node_uuid=user_node_uuid,
    )

    return ChatResponse(
        ok=True,
        action="response",
        message=f"(demo) Context outline:\n{outline}",
        contextToken=chat_request.contextToken or "new-context-token",
        data={
            "note": "Replace this with GPT tool-use logic that calls Shopware APIs.",
            "userNodeUuid": user_node_uuid,
            "inputWasVoice": was_voice,
            "intent": primary_goals.model_dump(),
        },
    )
    

# ------------------------------
# Dev server (optional)
# ------------------------------
if __name__ == "__main__":
    import uvicorn
    uvicorn.run("main:app", host="0.0.0.0", port=int(os.getenv("PORT", 8000)), reload=True)
